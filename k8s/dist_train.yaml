apiVersion: batch/v1
kind: Job
metadata:
  name: nmoe-dist-train
  labels:
    app: nmoe
    stage: dist-train
spec:
  completions: 2
  parallelism: 2
  completionMode: Indexed
  template:
    metadata:
      labels:
        app: nmoe
        stage: dist-train
    spec:
      restartPolicy: Never
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      runtimeClassName: nvidia
      tolerations:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
      # Anti-affinity to ensure pods land on different nodes
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: nmoe
                  stage: dist-train
              topologyKey: kubernetes.io/hostname
      containers:
        - name: nmoe
          image: xjdr/nmoe:train
          imagePullPolicy: Always
          command: ["/bin/bash"]
          args:
            - -c
            - |
              cd /workspace/nmoe
              . .venv/bin/activate

              # Get job index from k8s
              export RANK=${JOB_COMPLETION_INDEX:-0}
              export WORLD_SIZE=2
              export LOCAL_WORLD_SIZE=8
              export MASTER_ADDR=${MASTER_ADDR:-nmoe-dist-train-0}
              export MASTER_PORT=29500

              echo "=== Multi-Node Training ==="
              echo "RANK: $RANK"
              echo "WORLD_SIZE: $WORLD_SIZE"
              echo "MASTER_ADDR: $MASTER_ADDR"

              # Wait for master to be ready if we're not rank 0
              if [ "$RANK" != "0" ]; then
                echo "Waiting for master..."
                sleep 10
              fi

              # Launch training
              torchrun \
                --nproc_per_node=8 \
                --nnodes=$WORLD_SIZE \
                --node_rank=$RANK \
                --master_addr=$MASTER_ADDR \
                --master_port=$MASTER_PORT \
                -m nmoe.train /data/configs/${CONFIG_NAME:-moonlight.toml}
          env:
            - name: PYTORCH_CUDA_ALLOC_CONF
              value: "expandable_segments:True"
            - name: NCCL_DEBUG
              value: "WARN"
            - name: NCCL_IB_DISABLE
              value: "0"
            - name: NCCL_NET_GDR_LEVEL
              value: "5"
          securityContext:
            capabilities:
              add:
                - IPC_LOCK
          volumeMounts:
            - name: data
              mountPath: /data
            - name: dshm
              mountPath: /dev/shm
          resources:
            requests:
              nvidia.com/gpu: 8
            limits:
              nvidia.com/gpu: 8
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: nmoe-data
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 64Gi
