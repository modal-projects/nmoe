# 50M Dense Model - HP Tuning Base
#
# Purpose: Small-scale HP tuning for Complete(d)P transfer validation.
# This is the starting point for the transfer chain:
#   50M dense → 500M dense → 500M MoE → 3B MoE
#
# Architecture (~50M params):
#   - Embedding: 2 × 50304 × 384 = 38.6M
#   - Layers: 6 × 12 × 384² = 10.6M
#   - Total: ~49.2M parameters
#
# Run:
#   torchrun --nproc_per_node=8 -m nmoe.train configs/scaling/50m_dense.toml

preset = "scaling_50m_dense"
experiment_id = "scaling_50m_dense"

# Tokenizer (GPT-2 for speedrun compatibility)
tokenizer = "gpt2"
vocab_size = 50304  # Padded for Quack CE
eos_token_id = 50256
loss_mask_eos = false

# Data
data_path = "/data/fineweb10B_gpt2_npy/train"
validation_enabled = true
validation_data_path = "/data/fineweb10B_gpt2_npy/valid"
validation_every = 100
validation_steps = 10

# =============================================================================
# Model Architecture (~50M params)
# =============================================================================
dim = 384
n_layers = 6
n_heads = 6
inter_dim = 1536  # 4x dim

# Dense-only
n_dense_layers = 6

# Attention (MLA for consistency with larger models)
attn = "mla"
q_lora_rank = 768    # 2x dim
kv_lora_rank = 256
qk_nope_head_dim = 64
qk_rope_head_dim = 32
v_head_dim = 64

# RoPE
rope_theta = 10000.0
max_position_embeddings = 2048

# Normalization
rms_norm_eps = 1e-6

# =============================================================================
# Training (short runs for HP search)
# =============================================================================
dtype = "bf16"
seq_len = 1024
batch_size = 64  # 64 × 1024 = 65K tokens/step
steps = 2000     # ~131M tokens (enough for HP signal)

# =============================================================================
# Optimizer (AdamW - baseline for tuning)
# =============================================================================
use_muon = false

# Calibrated baseline: transfers to DeepSeek 3B @ 8.6e-4
# Derivation: 8.6e-4 / 0.6564 = 1.31e-3
# where 0.6564 = (384/1280)^0.35 × (6/12)^0.58 × (10.67/1)^0.17
lr_dense = 1.31e-3
lr_expert = 1.31e-3
lr_router = 1.31e-3
weight_decay = 0.1
adam_beta1 = 0.9
adam_beta2 = 0.95
adam_eps = 1e-8

# =============================================================================
# Schedule (simple for HP search)
# =============================================================================
warmup_steps = 200
hold_tokens = 80_000_000   # ~60% hold
decay_tokens = 51_200_000  # ~40% decay
decay_floor = 1.31e-4  # 0.1× of base LR

# =============================================================================
# Checkpointing
# =============================================================================
resume = false
checkpoint_every = 500
log_every = 10
