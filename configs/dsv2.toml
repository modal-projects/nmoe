# DSv2: DeepSeek-V2 236B/22B MoE (2 nodes Ã— 8 GPUs = 16 GPUs)
#
# Usage:
#   # Multi-node with torchrun (see k8s/dist-train.yaml)
#   torchrun --nnodes=2 --nproc_per_node=8 ... -m nmoe.train configs/dsv2.toml
#
# Reference: https://github.com/deepseek-ai/DeepSeek-V2

preset = "dsv2"
experiment_id = "dsv2_dev"

# =============================================================================
# Model Architecture
# =============================================================================
vocab_size = 163840
tokenizer = "o200k_harmony"
dim = 5120
inter_dim = 12288
moe_inter_dim = 1536
n_layers = 60
n_dense_layers = 1
n_heads = 128
n_routed_experts = 160
n_shared_experts = 2
n_activated_experts = 6
route_scale = 16.0

# MLA
q_lora_rank = 1536
kv_lora_rank = 512
qk_nope_head_dim = 128
qk_rope_head_dim = 64
v_head_dim = 128

# RoPE
max_position_embeddings = 8192
rope_theta = 50000.0
rms_norm_eps = 1e-5

# =============================================================================
# Data
# =============================================================================
data_path = "/data/fineweb_edu"

# =============================================================================
# Optimizer
# =============================================================================
lr_dense = 2.0e-4
lr_router = 2.0e-4
lr_expert = 2.0e-4
weight_decay = 0.1
adam_beta1 = 0.9
adam_beta2 = 0.95

# =============================================================================
# Training (16 GPUs, batch_size is global)
# =============================================================================
steps = 2000
dtype = "nvfp4"
batch_size = 128
seq_len = 4096
resume = false
