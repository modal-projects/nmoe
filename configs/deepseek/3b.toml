# DeepSeek-V3 3B Configuration
# Based on DeepSeek's published hyperparameters (Table 5)
#
# Run:
#   torchrun --nproc_per_node=8 -m nmoe.train configs/deepseek/3b.toml

preset = "deepseek_3b"
experiment_id = "deepseek_3b"

# Tokenizer (o200k_harmony - matches moonlight)
vocab_size = 201088
tokenizer = "o200k_harmony"
eos_token_id = 199999

# Data (OLMo3 mixture with 5.93T tokens available)
mixture_toml = "configs/mixtures/olmo3_1025.toml"
flow_profiles_toml = "configs/flow_profiles.toml"
flow_mode = "full_train"

# =============================================================================
# Model Architecture (DeepSeek 3B: 612M active, 2.97B total)
# =============================================================================
dim = 1280
n_layers = 12
n_heads = 16
inter_dim = 896  # Dense FFN dimension (same as moe_inter_dim for DeepSeek)

# MoE Configuration
n_dense_layers = 1  # Leading dense layers
n_routed_experts = 64
n_activated_experts = 6
n_shared_experts = 2
moe_inter_dim = 896

# Attention (MLA)
attn = "mla"
q_lora_rank = 1536
kv_lora_rank = 512
qk_nope_head_dim = 128
qk_rope_head_dim = 64
v_head_dim = 128

# RoPE
rope_theta = 10000.0
max_position_embeddings = 4096

# Normalization
rms_norm_eps = 1e-20

# =============================================================================
# Training
# =============================================================================
seq_len = 4096
batch_size = 320
steps = 30000

# Precision
dtype = "bf16"

# =============================================================================
# Optimizer (AdamW - DeepSeek settings)
# =============================================================================
use_muon = false  # DeepSeek uses pure AdamW

lr_dense = 8.6e-4
lr_expert = 8.6e-4
lr_router = 8.6e-4
weight_decay = 0.1
adam_beta1 = 0.9
adam_beta2 = 0.95
adam_eps = 1e-20

# =============================================================================
# Schedule (Step decay at 80% and 90% of training)
# =============================================================================
warmup_steps = 2000

# Step decay: 0.316x at 80%, 0.1x at 90%
# For 30000 steps: decay at step 24000 and 27000
# Total tokens: 30000 * 320 * 4096 = 39.3B
hold_tokens = 31_457_280_000    # 80% of total
decay_tokens = 7_864_320_000    # 20% remaining
decay_floor = 8.6e-5         # 0.1x of base LR

# =============================================================================
# Load Balancing (Loss-Free method)
# =============================================================================
# DeepSeek uses Loss-Free balancing (Wang et al., 2024)
# router_bias_update_rate handles this
router_bias_update_rate = 1e-4
aux_loss_alpha = 0.0  # No aux loss - Loss-Free method

# =============================================================================
# mHC (Multi-stream HyperConnections) - if implemented
# =============================================================================
# mhc_expansion_rate = 4
# mhc_gating_init = 0.01
# mhc_sinkhorn_iterations = 20

# =============================================================================
# Checkpointing
# =============================================================================
resume = false
checkpoint_every = 1000
log_every = 10
