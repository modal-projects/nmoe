# nmoe.serve benchmark configuration (LMSYS-style decode throughput)
# Usage: python -m nmoe.serve.launch --config configs/serve/benchmark_lmsys.toml

# Model (use env var for path portability)
model_path = "${NMOE_MODEL_PATH}"
model_family = "deepseek"

# Scheduling (tuned for decode benchmark)
max_batch_size = 256
max_prefill_tokens = 8192
max_seq_len = 32768

# Memory (reduced for benchmark - 4k pages sufficient for BS=256 @ ctx=2k)
gpu_memory_utilization = 0.9
num_pages = 4096

# KV cache layout (DeepSeek-V3 MLA defaults)
[kv_layout]
kv_lora_rank = 512
qk_rope_head_dim = 64
num_layers = 61
page_size = 64

# Replica configuration (8 GPUs, single "both" replica for EP=8)
[[replicas]]
replica_id = 0
gpus = [0, 1, 2, 3, 4, 5, 6, 7]
role = "both"

# API server (not used in benchmark mode, but required by config)
host = "0.0.0.0"
port = 8000
