# nmoe Speedrun: MoE-64
#
# 64 routed experts, top-6 + 2 shared = 8 active per token.
# Active params matched to dense baseline.
#
# Run: n speedrun moe

preset = "speedrun_moe"
experiment_id = "speedrun_moe_64"

# Tokenizer (GPT-2, padded to 128)
tokenizer = "gpt2"
vocab_size = 50304
eos_token_id = 50256
loss_mask_eos = false

# Data - paths set dynamically by CLI via --data_root
# (train: {data_root}/speedrun/train, val: {data_root}/speedrun/val)
validation_enabled = true
validation_every = 128
validation_steps = 20

# =============================================================================
# Model: MoE-64
# =============================================================================
dim = 768
n_layers = 12
n_heads = 12
inter_dim = 3072

# MoE: 64 routed experts, top-6 routed + 2 shared = 8 total/token
n_dense_layers = 1
n_routed_experts = 64
n_shared_experts = 2
n_activated_experts = 6
moe_inter_dim = 384  # 8 Ã— 384 = 3072 = dense inter_dim

attn = "sdpa"

# =============================================================================
# Training
# =============================================================================
dtype = "nvfp4"
seq_len = 2048
batch_size = 256    # 524K tokens/step
steps = 50000       # High limit - early stopping at target_loss=3.28

# =============================================================================
# Optimizer
# =============================================================================
use_muon = false
lr_dense = 1.8e-3
lr_expert = 1.8e-3
lr_router = 1.8e-3
weight_decay = 0.1
adam_beta1 = 0.9
adam_beta2 = 0.95
adam_eps = 1e-8

# =============================================================================
# Schedule (WSD) - Open-ended for speedrun (hold until target_loss reached)
# =============================================================================
warmup_steps = 256
hold_tokens = 100_000_000_000_000  # 100T - effectively infinite (stay at peak LR)
decay_tokens = 0                    # No decay for speedruns
decay_floor = 1e-4                  # Safety floor (won't be reached)

# =============================================================================
# Load Balancing (Loss-Free)
# =============================================================================
router_bias_update_rate = 1e-4
aux_loss_alpha = 0.0

# =============================================================================
# Checkpointing
# =============================================================================
resume = false
checkpoint_every = 999999999
log_every = 100

target_loss = 3.28
