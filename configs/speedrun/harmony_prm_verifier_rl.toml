# Speedrun: Harmony PRM verifier RL sanity
#
# Purpose: exercise the Harmony-first verifier/PRM step-label training loop
# without MoE/RDEP complexity (dense-only model).
#
# Run (8 GPUs):
#   torchrun --nproc_per_node=8 -m nmoe.rl.train_verifier configs/speedrun/harmony_prm_verifier_rl.toml \
#     --prm_source=prm800k --prm_split=train[:1024] --prm_max_examples=4096 --mode=step
#
# Run (1 GPU):
#   python -m nmoe.rl.train_verifier configs/speedrun/harmony_prm_verifier_rl.toml \
#     --prm_source=prm800k --prm_split=train[:1024] --prm_max_examples=4096 --mode=step --batch_size=1

preset = "speedrun_harmony_prm_verifier_rl"
experiment_id = "speedrun_harmony_prm_verifier_rl"

# Harmony tokenizer contract (must include <|start|>/<|end|>/<|message|>/<|channel|> as single tokens)
tokenizer = "o200k_harmony"
vocab_size = 201088
eos_token_id = 199999

# Dense-only tiny model (no MoE/RDEP): set n_dense_layers == n_layers
dim = 256
n_layers = 4
n_heads = 4
inter_dim = 1024
n_dense_layers = 4

# Training/RL
rl_enabled = true
rl_algorithm = "grpo"
batch_size = 8
rl_group_size = 4
steps = 2
seq_len = 256
dtype = "bf16"

# Rollout settings
rl_max_new_tokens = 96
rl_temperature = 1.0
rl_top_p = 1.0
rl_updates_per_batch = 1

# GRPO defaults
rl_clip_eps = 0.2
rl_kl_coef = 0.001
rl_kl_type = "k3"
rl_normalize_mean = true
rl_normalize_std = false
rl_length_norm_constant = true
rl_filter_zero_std = true

# Optimizer
use_muon = false
lr_dense = 1e-5
lr_router = 1e-5
lr_expert = 1e-5
weight_decay = 0.0
warmup_steps = 0
hold_tokens = 1000000000
decay_tokens = 1
decay_floor = 1e-6

# Checkpointing/logging (off for speedrun)
checkpoint_dir = "/data/checkpoints"
resume = false
checkpoint_every = 999999999
log_every = 1

