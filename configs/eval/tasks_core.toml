# CORE evaluation suite (nanochat/DCLM parity)
# Schema mirrors configs/eval/tasks.toml: an array of [[task]] tables
# Fields:
#   name      - human-readable task name
#   source    - dataset locator in the form "hf:<dataset>:<subset>:<split>" or other schemes
#   fewshot   - number of in-context examples (0 unless specified)
#   scorer    - one of: choices | span | unittest | judge
#   (optional) answer_field, tau_keep, etc.

[[task]]
label = "ARC-Easy"
source = "hf:allenai/ai2_arc:ARC-Easy:test"
fewshot = 0
scorer = "choices"

[[task]]
label ="ARC-Challenge"
source = "hf:allenai/ai2_arc:ARC-Challenge:test"
fewshot = 0
scorer = "choices"

[[task]]
label ="HellaSwag"
source = "hf:Rowan/hellaswag:validation"
fewshot = 0
scorer = "choices"

[[task]]
label ="MMLU"
source = "hf:cais/mmlu:all:dev"
fewshot = 5
scorer = "choices"

[[task]]
label ="OpenBookQA"
source = "hf:allenai/openbookqa:main:test"
fewshot = 0
scorer = "choices"

[[task]]
label ="WinoGrande"
source = "hf:allenai/winogrande:winogrande_debiased:validation"
fewshot = 0
scorer = "choices"

[[task]]
label ="BoolQ"
source = "hf:google/boolq:validation"
fewshot = 0
scorer = "choices"

[[task]]
label ="COPA"
source = "hf:super_glue:copa:validation"
fewshot = 0
scorer = "choices"


[[task]]
label ="SQuAD v1.1"
source = "hf:squad:plain_text:validation"
fewshot = 0
scorer = "span"
answer_field = "answers"  # list of {text: str}
