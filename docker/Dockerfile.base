# Use NVIDIA cuDNN with CUDA 12.8 devel on Ubuntu 24.04 as base image
FROM nvidia/cuda:12.8.0-cudnn-devel-ubuntu24.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}
ENV TORCH_CUDA_ARCH_LIST="10.0a"
ENV CUTLASS_NVCC_ARCHS="100a"
ENV UV_HTTP_TIMEOUT=900 UV_HTTP_RETRIES=5 UV_CONCURRENT_DOWNLOADS=1 UV_CONCURRENT_INSTALLS=1

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    cmake \
    git \
    curl \
    wget \
    ninja-build \
    zlib1g-dev \
    libxml2-dev \
    python3.12-dev \
    && rm -rf /var/lib/apt/lists/*

# Install uv for fast Python package management
RUN curl -LsSf https://astral.sh/uv/install.sh | sh
ENV PATH="/root/.local/bin:${PATH}"

# Set working directory
WORKDIR /workspace/nmoe

# Create virtual environment with Python 3.12 explicitly
RUN uv venv --python python3.12

# Install PyTorch nightly with CUDA 12.8
RUN uv pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu128
RUN uv pip install tiktoken pybind11 openai_harmony numpy nvidia-cutlass-dsl==4.3.1 cuda-python==12.8.0 duckdb

# Clone and build Triton from source (pinned commit)
RUN git clone https://github.com/triton-lang/triton.git triton && \
    cd triton && git checkout bad25767a03107bc23e066d94aca489dc86ca70c && \
    cd .. && uv pip install -e ./triton

# Clone FlashAttention (pinned) and keep only the CuTe implementation (FA4).
# We vendor CuTe sources into /workspace/nmoe/third_party/flash_attn/cute.
RUN mkdir -p third_party/flash_attn_upstream && \
    cd third_party/flash_attn_upstream && \
    git init && \
    git remote add origin https://github.com/Dao-AILab/flash-attention.git && \
    git sparse-checkout init --cone && \
    git sparse-checkout set flash_attn/cute && \
    git fetch --depth 1 origin ac9b5f107f2f19cd0ca6e01548d20d072a46335c && \
    git checkout --detach FETCH_HEAD && \
    cd /workspace/nmoe && \
    rm -rf third_party/flash_attn/cute && \
    mkdir -p third_party/flash_attn && \
    cp -a third_party/flash_attn_upstream/flash_attn/cute third_party/flash_attn/cute && \
    rm -rf third_party/flash_attn_upstream && \
    echo "✓ Vendored FlashAttention CuTe (FA4, pinned)"

# Clone FlashMLA (pinned) and its CUTLASS submodule.
# We keep only the SM100 dense prefill backward sources + CUTLASS headers.
RUN git clone https://github.com/deepseek-ai/FlashMLA.git third_party/flashmla && \
    cd third_party/flashmla && \
    git checkout 1408756a88e52a25196b759eaf8db89d2b51b5a1 && \
    git submodule update --init --recursive && \
    rm -rf .git && \
    find . -name .git -o -name .gitmodules -exec rm -rf {} + && \
    echo "✓ Cloned FlashMLA (dense SM100 prefill bwd only)"
