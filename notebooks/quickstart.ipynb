{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, tomllib, torch, torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from dataclasses import asdict\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nmoe.config import Config\n",
    "from nmoe.model import Transformer\n",
    "from nmoe.data.loader import build_loader\n",
    "from nmoe.opt import build_optimizer, update_lr, step\n",
    "from nmoe.checkpoint import Checkpointer, load_checkpoint\n",
    "from nmoe import runtime\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MEMORY\n",
    "# ============================================================================\n",
    "\n",
    "def mem_stats():\n",
    "    \"\"\"Current GPU memory state.\"\"\"\n",
    "    alloc = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    peak = torch.cuda.max_memory_allocated() / 1e9\n",
    "    return {'allocated_gb': alloc, 'reserved_gb': reserved, 'peak_gb': peak}\n",
    "\n",
    "def param_memory(model):\n",
    "    \"\"\"Memory used by parameters (not activations).\"\"\"\n",
    "    total = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    return total / 1e9\n",
    "\n",
    "def optimizer_memory(optimizer):\n",
    "    \"\"\"Memory used by optimizer state (adam moments etc).\"\"\"\n",
    "    total = 0\n",
    "    for state in optimizer.state.values():\n",
    "        for v in state.values():\n",
    "            if torch.is_tensor(v):\n",
    "                total += v.numel() * v.element_size()\n",
    "    return total / 1e9\n",
    "\n",
    "def activation_memory(model, x):\n",
    "    \"\"\"Estimate activation memory from a forward pass.\"\"\"\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    baseline = torch.cuda.memory_allocated()\n",
    "    with torch.no_grad():\n",
    "        _ = model(x)\n",
    "    torch.cuda.synchronize()\n",
    "    return (torch.cuda.max_memory_allocated() - baseline) / 1e9\n",
    "\n",
    "def memory_breakdown(model, optimizer, x):\n",
    "    \"\"\"Full memory attribution.\"\"\"\n",
    "    return {\n",
    "        'params_gb': param_memory(model),\n",
    "        'optimizer_gb': optimizer_memory(optimizer) if optimizer else 0,\n",
    "        'activations_gb': activation_memory(model, x),\n",
    "        **mem_stats()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TIMING\n",
    "# ============================================================================\n",
    "\n",
    "def time_forward_backward(model, x, y, vocab_size, warmup=3, iters=10):\n",
    "    \"\"\"Time forward + backward pass.\"\"\"\n",
    "    for _ in range(warmup):\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    start.record()\n",
    "    for _ in range(iters):\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    ms = start.elapsed_time(end) / iters\n",
    "    tokens = x.numel()\n",
    "    return {'ms': ms, 'tokens_per_sec': tokens / (ms / 1000)}\n",
    "\n",
    "def profile_kernels(model, x, y, vocab_size):\n",
    "    \"\"\"Get top CUDA kernels by time.\"\"\"\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CUDA]) as prof:\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        torch.cuda.synchronize()\n",
    "    return prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MOE ROUTING\n",
    "# ============================================================================\n",
    "\n",
    "class RouterProbe:\n",
    "    \"\"\"Capture routing decisions during forward pass.\"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.stats = defaultdict(list)\n",
    "        self._hooks = []\n",
    "        \n",
    "    def _hook(self, layer_idx):\n",
    "        def fn(module, input, output):\n",
    "            if hasattr(module, 'gate') and hasattr(module.gate, 'weight'):\n",
    "                # Assuming output contains routing info or we can compute it\n",
    "                x = input[0] if isinstance(input, tuple) else input\n",
    "                with torch.no_grad():\n",
    "                    scores = F.softmax(x @ module.gate.weight.T, dim=-1)\n",
    "                    topk = scores.topk(module.n_activated, dim=-1)\n",
    "                    expert_counts = torch.bincount(topk.indices.view(-1), minlength=module.n_experts)\n",
    "                    self.stats[layer_idx].append({\n",
    "                        'counts': expert_counts.cpu(),\n",
    "                        'entropy': -(scores * scores.log().nan_to_num()).sum(-1).mean().item(),\n",
    "                        'max_prob': scores.max(-1).values.mean().item(),\n",
    "                    })\n",
    "        return fn\n",
    "    \n",
    "    def attach(self):\n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            if hasattr(layer, 'moe'):\n",
    "                h = layer.moe.register_forward_hook(self._hook(i))\n",
    "                self._hooks.append(h)\n",
    "        return self\n",
    "    \n",
    "    def detach(self):\n",
    "        for h in self._hooks:\n",
    "            h.remove()\n",
    "        self._hooks = []\n",
    "        \n",
    "    def summary(self):\n",
    "        rows = []\n",
    "        for layer_idx, records in self.stats.items():\n",
    "            counts = torch.stack([r['counts'] for r in records]).float().mean(0)\n",
    "            rows.append({\n",
    "                'layer': layer_idx,\n",
    "                'entropy': np.mean([r['entropy'] for r in records]),\n",
    "                'max_prob': np.mean([r['max_prob'] for r in records]),\n",
    "                'load_std': counts.std().item(),\n",
    "                'dead_experts': (counts == 0).sum().item(),\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "    \n",
    "    def plot_load(self, layers=None):\n",
    "        layers = layers or list(self.stats.keys())[:4]\n",
    "        fig, axes = plt.subplots(1, len(layers), figsize=(4*len(layers), 3))\n",
    "        if len(layers) == 1: axes = [axes]\n",
    "        for ax, layer_idx in zip(axes, layers):\n",
    "            counts = torch.stack([r['counts'] for r in self.stats[layer_idx]]).float().mean(0)\n",
    "            ax.bar(range(len(counts)), counts.numpy())\n",
    "            ax.set_title(f'Layer {layer_idx}')\n",
    "            ax.set_xlabel('Expert')\n",
    "        plt.tight_layout()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GRADIENT HEALTH\n",
    "# ============================================================================\n",
    "\n",
    "def grad_norms(model, per_layer=False):\n",
    "    \"\"\"Gradient L2 norms.\"\"\"\n",
    "    norms = {}\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.grad is not None:\n",
    "            norms[name] = p.grad.norm().item()\n",
    "    s = pd.Series(norms)\n",
    "    if per_layer:\n",
    "        return s\n",
    "    return {'max': s.max(), 'min': s.min(), 'mean': s.mean(), 'std': s.std()}\n",
    "\n",
    "def grad_flow(model):\n",
    "    \"\"\"Plot gradient flow through layers.\"\"\"\n",
    "    norms = []\n",
    "    names = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.grad is not None and 'weight' in name:\n",
    "            norms.append(p.grad.norm().item())\n",
    "            names.append(name.replace('.weight', ''))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(max(12, len(norms)//4), 4))\n",
    "    ax.bar(range(len(norms)), norms)\n",
    "    ax.set_xticks(range(len(norms)))\n",
    "    ax.set_xticklabels(names, rotation=90, fontsize=6)\n",
    "    ax.set_ylabel('Gradient Norm')\n",
    "    ax.set_yscale('log')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def check_nan_inf(model):\n",
    "    \"\"\"Find NaN/Inf in params or grads.\"\"\"\n",
    "    issues = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if torch.isnan(p).any():\n",
    "            issues.append(f'{name}: NaN in param')\n",
    "        if torch.isinf(p).any():\n",
    "            issues.append(f'{name}: Inf in param')\n",
    "        if p.grad is not None:\n",
    "            if torch.isnan(p.grad).any():\n",
    "                issues.append(f'{name}: NaN in grad')\n",
    "            if torch.isinf(p.grad).any():\n",
    "                issues.append(f'{name}: Inf in grad')\n",
    "    return issues or ['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CHECKPOINTS\n",
    "# ============================================================================\n",
    "\n",
    "def list_checkpoints(base='/data/checkpoints'):\n",
    "    \"\"\"List available checkpoints.\"\"\"\n",
    "    p = Path(base)\n",
    "    if not p.exists():\n",
    "        return []\n",
    "    return sorted([d.name for d in p.iterdir() if d.is_dir()])\n",
    "\n",
    "def peek_checkpoint(path):\n",
    "    \"\"\"Inspect checkpoint contents without loading model.\"\"\"\n",
    "    p = Path(path)\n",
    "    info = {}\n",
    "    \n",
    "    meta = p / 'meta.pt'\n",
    "    if meta.exists():\n",
    "        m = torch.load(meta, map_location='cpu', weights_only=False)\n",
    "        info['step'] = m.get('step')\n",
    "        info['tokens'] = m.get('tokens_seen')\n",
    "        info['config_hash'] = m.get('config_fingerprint', '')[:8]\n",
    "    \n",
    "    model_pt = p / 'model.pt'\n",
    "    if model_pt.exists():\n",
    "        info['model_size_gb'] = model_pt.stat().st_size / 1e9\n",
    "    \n",
    "    return info\n",
    "\n",
    "def diff_configs(cfg1, cfg2):\n",
    "    \"\"\"Show differences between two configs.\"\"\"\n",
    "    d1, d2 = asdict(cfg1), asdict(cfg2)\n",
    "    diffs = []\n",
    "    for k in set(d1) | set(d2):\n",
    "        v1, v2 = d1.get(k), d2.get(k)\n",
    "        if v1 != v2:\n",
    "            diffs.append({'key': k, 'cfg1': v1, 'cfg2': v2})\n",
    "    return pd.DataFrame(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# QUICK ABLATIONS\n",
    "# ============================================================================\n",
    "\n",
    "def quick_train(cfg_overrides, steps=20, log_every=5):\n",
    "    \"\"\"Run a few steps, return loss curve.\"\"\"\n",
    "    with open('configs/moonlet.toml', 'rb') as f:\n",
    "        cfg = Config(**{**tomllib.load(f), **cfg_overrides, 'steps': steps})\n",
    "    \n",
    "    rank, world = runtime.init(cfg.seed)\n",
    "    loader, plan = build_loader(cfg, rank, world)\n",
    "    model = Transformer(cfg).cuda()\n",
    "    model.init_weights()\n",
    "    model.train()\n",
    "    optimizer, dense_groups = build_optimizer(model, cfg)\n",
    "    \n",
    "    losses = []\n",
    "    for i in range(steps):\n",
    "        x, y = loader.next()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, cfg.vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        step(model, optimizer, dense_groups, {}, cfg, world)\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        if (i + 1) % log_every == 0:\n",
    "            print(f'[{i+1}/{steps}] loss={loss.item():.4f}')\n",
    "    \n",
    "    del model, optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "    runtime.finalize()\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def compare_ablations(configs, steps=20):\n",
    "    \"\"\"Run multiple configs, plot loss curves.\"\"\"\n",
    "    results = {}\n",
    "    for name, overrides in configs.items():\n",
    "        print(f'\\n=== {name} ===')\n",
    "        results[name] = quick_train(overrides, steps=steps)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    for name, losses in results.items():\n",
    "        ax.plot(losses, label=name)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    return fig, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open('configs/moonlet.toml', 'rb') as f:\n",
    "    cfg = Config(**tomllib.load(f))\n",
    "\n",
    "# Init\n",
    "rank, world = runtime.init(cfg.seed)\n",
    "model = Transformer(cfg).cuda()\n",
    "model.init_weights()\n",
    "model.train()\n",
    "optimizer, dense_groups = build_optimizer(model, cfg)\n",
    "\n",
    "# Dummy batch\n",
    "x = torch.randint(0, cfg.vocab_size, (cfg.batch_size, cfg.seq_len), device='cuda')\n",
    "y = torch.randint(0, cfg.vocab_size, (cfg.batch_size, cfg.seq_len), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del model, optimizer, x, y\n",
    "torch.cuda.empty_cache()\n",
    "runtime.finalize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
