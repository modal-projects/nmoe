# Third-Party Notices

This repository includes or depends on third-party software. Below are the licenses and acknowledgments for each.

---

## PyTorch

BSD 3-Clause License

Copyright (c) 2016-present, Facebook Inc.

https://github.com/pytorch/pytorch/blob/main/LICENSE

---

## FlashAttention

BSD 3-Clause License

Copyright (c) 2022, Tri Dao, Dan Fu, Khaled K. Elmeleegy, and others.

https://github.com/Dao-AILab/flash-attention/blob/main/LICENSE

---

## FlashMLA

MIT License

Copyright (c) 2024 DeepSeek

https://github.com/deepseek-ai/FlashMLA/blob/main/LICENSE

---

## NVIDIA CUTLASS

BSD 3-Clause License

Copyright (c) 2017-2024, NVIDIA CORPORATION.

https://github.com/NVIDIA/cutlass/blob/main/LICENSE.txt

---

## TensorRT-LLM

Apache License 2.0

https://github.com/NVIDIA/TensorRT-LLM

---

## NVIDIA NVSHMEM

NVIDIA Software License Agreement (proprietary)

https://docs.nvidia.com/nvshmem/

---

## vLLM

Apache License 2.0

Portions of `nmoe/serve/benchmark_router_vllm_grouped_topk.cu` are derived from vLLM.

https://github.com/vllm-project/vllm

---

## SGLang

Apache License 2.0

Portions of `nmoe/serve/kernels/sglang_moe_fused_gate.cu` are derived from SGLang.

https://github.com/sgl-project/sglang

---

## Triton

MIT License

Copyright (c) 2021-2024 OpenAI

https://github.com/triton-lang/triton/blob/main/LICENSE

---

## tiktoken

MIT License

Copyright (c) 2022 OpenAI

https://github.com/openai/tiktoken/blob/main/LICENSE

---

## flash-linear-attention

MIT License

Copyright (c) 2024 Songlin Yang

Portions of `nmoe/triton/kda.py` are derived from this project.

https://github.com/sustcsonglin/flash-linear-attention/blob/main/LICENSE

---

## Additional Acknowledgments

- DeepSeek-V2/V3 architecture papers for MLA and MoE design inspiration
- NVIDIA for B200 GPU architecture and CUDA ecosystem
